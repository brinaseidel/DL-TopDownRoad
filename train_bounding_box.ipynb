{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bounding Box model\n",
    "\n",
    "This notebook trains a model to take in six images from the car's point of view, and output a bird's eye view of the bouding boxes around surrounding objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#!conda install -c conda-forge opencv -y\n",
    "import cv2 \n",
    "from yolov3 import *\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/bs3743/DL-TopDownRoad/data'\n",
    "annotation_csv = '/scratch/bs3743/DL-TopDownRoad/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the labeled training data, and split into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenes from 106 - 133 are labeled\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes: 21 \n",
      "Val scenes: 7\n"
     ]
    }
   ],
   "source": [
    "# Split 75/25 into training and validation\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)\n",
    "#print(torch.stack(sample)[:, 1, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "\n",
    "config = \"yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 2.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.0001,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.00005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25777e+07 parameters, 6.25777e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "model = Darknet(config, verbose=False).to(device)\n",
    "#ONNX_EXPORT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize optimizer\n",
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(pg0, lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "\n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "\n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function\n",
    "def evaluate(val_loader, model, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "         \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "        # Calculate loss\n",
    "        #print(output[0].shape)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 10.482500\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 4.734860\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 5.984637\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 9.504921\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 3.962355\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 3.387667\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 3.447982\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 6.019710\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 7.455266\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 6.022396\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 3.975966\n",
      "\tTrain Epoch: 0 [2200/2646 (83%)]\tLoss: 3.386369\n",
      "\tTrain Epoch: 0 [2400/2646 (91%)]\tLoss: 4.544101\n",
      "\tTrain Epoch: 0 [2600/2646 (98%)]\tLoss: 4.210502\n",
      "Evaluating after Epoch 0:\n",
      "Val loss is tensor([5.31724])\n",
      "\tTrain Epoch: 1 [0/2646 (0%)]\tLoss: 5.233622\n",
      "\tTrain Epoch: 1 [200/2646 (8%)]\tLoss: 2.081358\n",
      "\tTrain Epoch: 1 [400/2646 (15%)]\tLoss: 4.674491\n",
      "\tTrain Epoch: 1 [600/2646 (23%)]\tLoss: 6.407662\n",
      "\tTrain Epoch: 1 [800/2646 (30%)]\tLoss: 5.409310\n",
      "\tTrain Epoch: 1 [1000/2646 (38%)]\tLoss: 3.889301\n",
      "\tTrain Epoch: 1 [1200/2646 (45%)]\tLoss: 5.204278\n",
      "\tTrain Epoch: 1 [1400/2646 (53%)]\tLoss: 7.684945\n",
      "\tTrain Epoch: 1 [1600/2646 (60%)]\tLoss: 2.851245\n",
      "\tTrain Epoch: 1 [1800/2646 (68%)]\tLoss: 4.561085\n",
      "\tTrain Epoch: 1 [2000/2646 (76%)]\tLoss: 7.786763\n",
      "\tTrain Epoch: 1 [2200/2646 (83%)]\tLoss: 6.098800\n",
      "\tTrain Epoch: 1 [2400/2646 (91%)]\tLoss: 3.669053\n",
      "\tTrain Epoch: 1 [2600/2646 (98%)]\tLoss: 3.486603\n",
      "Evaluating after Epoch 1:\n",
      "Val loss is tensor([4.97693])\n",
      "\tTrain Epoch: 2 [0/2646 (0%)]\tLoss: 2.186892\n",
      "\tTrain Epoch: 2 [200/2646 (8%)]\tLoss: 3.742745\n",
      "\tTrain Epoch: 2 [400/2646 (15%)]\tLoss: 4.547334\n",
      "\tTrain Epoch: 2 [600/2646 (23%)]\tLoss: 3.301547\n",
      "\tTrain Epoch: 2 [800/2646 (30%)]\tLoss: 3.623971\n",
      "\tTrain Epoch: 2 [1000/2646 (38%)]\tLoss: 2.562977\n",
      "\tTrain Epoch: 2 [1200/2646 (45%)]\tLoss: 2.723569\n",
      "\tTrain Epoch: 2 [1400/2646 (53%)]\tLoss: 3.746568\n",
      "\tTrain Epoch: 2 [1600/2646 (60%)]\tLoss: 7.091246\n",
      "\tTrain Epoch: 2 [1800/2646 (68%)]\tLoss: 3.858695\n",
      "\tTrain Epoch: 2 [2000/2646 (76%)]\tLoss: 3.790980\n",
      "\tTrain Epoch: 2 [2200/2646 (83%)]\tLoss: 3.657077\n",
      "\tTrain Epoch: 2 [2400/2646 (91%)]\tLoss: 2.740395\n",
      "\tTrain Epoch: 2 [2600/2646 (98%)]\tLoss: 4.737519\n",
      "Evaluating after Epoch 2:\n",
      "Val loss is tensor([4.94695])\n",
      "\tTrain Epoch: 3 [0/2646 (0%)]\tLoss: 7.497125\n",
      "\tTrain Epoch: 3 [200/2646 (8%)]\tLoss: 4.457232\n",
      "\tTrain Epoch: 3 [400/2646 (15%)]\tLoss: 3.295712\n",
      "\tTrain Epoch: 3 [600/2646 (23%)]\tLoss: 4.176288\n",
      "\tTrain Epoch: 3 [800/2646 (30%)]\tLoss: 2.444650\n",
      "\tTrain Epoch: 3 [1000/2646 (38%)]\tLoss: 3.168387\n",
      "\tTrain Epoch: 3 [1200/2646 (45%)]\tLoss: 3.236937\n",
      "\tTrain Epoch: 3 [1400/2646 (53%)]\tLoss: 4.989393\n",
      "\tTrain Epoch: 3 [1600/2646 (60%)]\tLoss: 3.202772\n",
      "\tTrain Epoch: 3 [1800/2646 (68%)]\tLoss: 4.370507\n",
      "\tTrain Epoch: 3 [2000/2646 (76%)]\tLoss: 3.501881\n",
      "\tTrain Epoch: 3 [2200/2646 (83%)]\tLoss: 3.646777\n",
      "\tTrain Epoch: 3 [2400/2646 (91%)]\tLoss: 4.345329\n",
      "\tTrain Epoch: 3 [2600/2646 (98%)]\tLoss: 8.757214\n",
      "Evaluating after Epoch 3:\n",
      "Val loss is tensor([4.88837])\n",
      "\tTrain Epoch: 4 [0/2646 (0%)]\tLoss: 6.169426\n",
      "\tTrain Epoch: 4 [200/2646 (8%)]\tLoss: 3.701352\n",
      "\tTrain Epoch: 4 [400/2646 (15%)]\tLoss: 5.883111\n",
      "\tTrain Epoch: 4 [600/2646 (23%)]\tLoss: 5.203564\n",
      "\tTrain Epoch: 4 [800/2646 (30%)]\tLoss: 3.033301\n",
      "\tTrain Epoch: 4 [1000/2646 (38%)]\tLoss: 3.629296\n",
      "\tTrain Epoch: 4 [1200/2646 (45%)]\tLoss: 3.004890\n",
      "\tTrain Epoch: 4 [1400/2646 (53%)]\tLoss: 2.515556\n",
      "\tTrain Epoch: 4 [1600/2646 (60%)]\tLoss: 3.007237\n",
      "\tTrain Epoch: 4 [1800/2646 (68%)]\tLoss: 3.037759\n",
      "\tTrain Epoch: 4 [2000/2646 (76%)]\tLoss: 2.593359\n",
      "\tTrain Epoch: 4 [2200/2646 (83%)]\tLoss: 6.111316\n",
      "\tTrain Epoch: 4 [2400/2646 (91%)]\tLoss: 3.679231\n",
      "\tTrain Epoch: 4 [2600/2646 (98%)]\tLoss: 2.981956\n",
      "Evaluating after Epoch 4:\n",
      "Val loss is tensor([4.91953])\n",
      "\tTrain Epoch: 5 [0/2646 (0%)]\tLoss: 5.152295\n",
      "\tTrain Epoch: 5 [200/2646 (8%)]\tLoss: 3.261356\n",
      "\tTrain Epoch: 5 [400/2646 (15%)]\tLoss: 3.993865\n",
      "\tTrain Epoch: 5 [600/2646 (23%)]\tLoss: 4.487677\n",
      "\tTrain Epoch: 5 [800/2646 (30%)]\tLoss: 3.262527\n",
      "\tTrain Epoch: 5 [1000/2646 (38%)]\tLoss: 4.652632\n",
      "\tTrain Epoch: 5 [1200/2646 (45%)]\tLoss: 3.780977\n",
      "\tTrain Epoch: 5 [1400/2646 (53%)]\tLoss: 5.852698\n",
      "\tTrain Epoch: 5 [1600/2646 (60%)]\tLoss: 4.738609\n",
      "\tTrain Epoch: 5 [1800/2646 (68%)]\tLoss: 2.483890\n",
      "\tTrain Epoch: 5 [2000/2646 (76%)]\tLoss: 5.206651\n",
      "\tTrain Epoch: 5 [2200/2646 (83%)]\tLoss: 4.232887\n",
      "\tTrain Epoch: 5 [2400/2646 (91%)]\tLoss: 3.897578\n",
      "\tTrain Epoch: 5 [2600/2646 (98%)]\tLoss: 3.680688\n",
      "Evaluating after Epoch 5:\n",
      "Val loss is tensor([4.96253])\n",
      "\tTrain Epoch: 6 [0/2646 (0%)]\tLoss: 2.678416\n",
      "\tTrain Epoch: 6 [200/2646 (8%)]\tLoss: 3.488343\n",
      "\tTrain Epoch: 6 [400/2646 (15%)]\tLoss: 2.645192\n",
      "\tTrain Epoch: 6 [600/2646 (23%)]\tLoss: 2.344493\n",
      "\tTrain Epoch: 6 [800/2646 (30%)]\tLoss: 3.125446\n",
      "\tTrain Epoch: 6 [1000/2646 (38%)]\tLoss: 3.417589\n",
      "\tTrain Epoch: 6 [1200/2646 (45%)]\tLoss: 2.699460\n",
      "\tTrain Epoch: 6 [1400/2646 (53%)]\tLoss: 1.646854\n",
      "\tTrain Epoch: 6 [1600/2646 (60%)]\tLoss: 3.275853\n",
      "\tTrain Epoch: 6 [1800/2646 (68%)]\tLoss: 1.935211\n",
      "\tTrain Epoch: 6 [2000/2646 (76%)]\tLoss: 2.816386\n",
      "\tTrain Epoch: 6 [2200/2646 (83%)]\tLoss: 4.113391\n",
      "\tTrain Epoch: 6 [2400/2646 (91%)]\tLoss: 4.319913\n",
      "\tTrain Epoch: 6 [2600/2646 (98%)]\tLoss: 5.712081\n",
      "Evaluating after Epoch 6:\n",
      "Val loss is tensor([4.88718])\n",
      "\tTrain Epoch: 7 [0/2646 (0%)]\tLoss: 7.648783\n",
      "\tTrain Epoch: 7 [200/2646 (8%)]\tLoss: 5.429219\n",
      "\tTrain Epoch: 7 [400/2646 (15%)]\tLoss: 2.966006\n",
      "\tTrain Epoch: 7 [600/2646 (23%)]\tLoss: 2.435974\n",
      "\tTrain Epoch: 7 [800/2646 (30%)]\tLoss: 8.308840\n",
      "\tTrain Epoch: 7 [1000/2646 (38%)]\tLoss: 5.847390\n",
      "\tTrain Epoch: 7 [1200/2646 (45%)]\tLoss: 2.173973\n",
      "\tTrain Epoch: 7 [1400/2646 (53%)]\tLoss: 3.173056\n",
      "\tTrain Epoch: 7 [1600/2646 (60%)]\tLoss: 3.463639\n",
      "\tTrain Epoch: 7 [1800/2646 (68%)]\tLoss: 2.763404\n",
      "\tTrain Epoch: 7 [2000/2646 (76%)]\tLoss: 6.177231\n",
      "\tTrain Epoch: 7 [2200/2646 (83%)]\tLoss: 2.351006\n",
      "\tTrain Epoch: 7 [2400/2646 (91%)]\tLoss: 6.082323\n",
      "\tTrain Epoch: 7 [2600/2646 (98%)]\tLoss: 5.066036\n",
      "Evaluating after Epoch 7:\n",
      "Val loss is tensor([4.96958])\n",
      "\tTrain Epoch: 8 [0/2646 (0%)]\tLoss: 4.031202\n",
      "\tTrain Epoch: 8 [200/2646 (8%)]\tLoss: 2.380836\n",
      "\tTrain Epoch: 8 [400/2646 (15%)]\tLoss: 4.193283\n",
      "\tTrain Epoch: 8 [600/2646 (23%)]\tLoss: 4.007797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-55265e51ba25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msixinput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate at the end of the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-be9ef5d2a27e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch, sixinput)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WARNING: non-finite loss.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Log progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "min_val_loss = np.inf\n",
    "#val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch, sixinput=False)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    #val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=False)\n",
    "    #print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model, sixinput=False)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, 'models/best_bounding_box.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)\n",
    "\n",
    "checkpoint = torch.load('models/best_bounding_box.pt')\n",
    "checkpoint['val_loss_hist'] = val_loss_hist\n",
    "#checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "torch.save(checkpoint, 'models/best_bounding_box.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(312.79037, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, target, road_image = iter(valloader).next()\n",
    "model.eval()\n",
    "sample = torch.stack(sample).to(device)\n",
    "\n",
    "# Make input the correct shape\n",
    "batch_size = sample.shape[0]\n",
    "sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "# Run through model\n",
    "optimizer.zero_grad()\n",
    "output = model(sample)\n",
    "output[0][1, :, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-max supression\n",
    "#Returns list of length batch_size, with each list element being a tensor of size nx6 (x1, y1, x2, y2, conf, cls) \n",
    "conf_thres = 0.07\n",
    "iou_thres = 0.6\n",
    "output = non_max_suppression(output[0], conf_thres, iou_thres,\n",
    "                                   multi_label=False, classes=None, )\n",
    "# Rescale from 256x306 to 80x80 (from -40 to 40)\n",
    "for i, preds in enumerate(output):\n",
    "    output[i][:, 0] = preds[:, 0] * 80/256 - 40\n",
    "    output[i][:, 1] = preds[:, 1] * 80/306 - 40\n",
    "    output[i][:, 2] = preds[:, 2] * 80/256 - 40\n",
    "    output[i][:, 3] = preds[:, 3] * 80/306 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store bounding boxes in the correct format\n",
    "bounding_boxes = torch.zeros((output[0].shape[0], 2, 4))\n",
    "#len(output[0])\n",
    "if output[0] != None:\n",
    "    for i in range(output[0].shape[0]):\n",
    "        # Get four corners\n",
    "        bounding_boxes[i, :, :] = torch.tensor([[output[0][i][0],  output[0][i][2], output[0][i][2],  output[0][i][0]],\n",
    "                                                [output[0][i][1],  output[0][i][1], output[0][i][3], output[0][i][3]]])\n",
    "# Truncate corners that are out of range\n",
    "bounding_boxes[bounding_boxes>40] = 40\n",
    "bounding_boxes[bounding_boxes<-40] = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(bounding_boxes):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "checkpoint = torch.load('models/best_road_mapper_lr10e-5_sixinput.pt')\n",
    "model = get_seg_model_sixinput(checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs threat score during training\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_loss_hist'], label=\"Val Loss\", linewidth=3)\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_threat_score_hist'], label=\"Val Threat Score\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "\n",
    "# Visualize results\n",
    "sample, target, road_image = iter(valloader).next()\n",
    "image = sample[0][None, :, :, :]\n",
    "#plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "#plt.axis('off')\n",
    "# True road image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_image[0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image\n",
    "output = model(image.to(device))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output.cpu().detach().numpy().squeeze(0), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image, with threshold\n",
    "output_binary = output[0, :, :] > 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output_binary.cpu().detach().numpy(), cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
