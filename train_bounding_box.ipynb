{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bounding Box model\n",
    "\n",
    "This notebook trains a model to take in six images from the car's point of view, and output a bird's eye view of the bouding boxes around surrounding objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#!conda install -c conda-forge opencv -y\n",
    "import cv2 \n",
    "from yolov3 import *\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/bs3743/DL-TopDownRoad/data'\n",
    "annotation_csv = '/scratch/bs3743/DL-TopDownRoad/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the labeled training data, and split into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenes from 106 - 133 are labeled\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes: 21 \n",
      "Val scenes: 7\n"
     ]
    }
   ],
   "source": [
    "# Split 75/25 into training and validation\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)\n",
    "#print(torch.stack(sample)[:, 1, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "\n",
    "config = \"yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 2.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 10e-5,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 5e-7,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25777e+07 parameters, 6.25777e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "model = Darknet(config, verbose=False).to(device)\n",
    "#ONNX_EXPORT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize optimizer\n",
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(pg0, lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "\n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "\n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function\n",
    "def evaluate(val_loader, model, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "         \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "        # Calculate loss\n",
    "        #print(output[0].shape)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 10.482500\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 4.753625\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 5.928028\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 9.264941\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 3.878227\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 3.273087\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 3.464520\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 6.097253\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 7.616407\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 6.397943\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 3.962619\n",
      "\tTrain Epoch: 0 [2200/2646 (83%)]\tLoss: 3.332586\n",
      "\tTrain Epoch: 0 [2400/2646 (91%)]\tLoss: 4.565530\n",
      "\tTrain Epoch: 0 [2600/2646 (98%)]\tLoss: 4.344994\n",
      "Evaluating after Epoch 0:\n",
      "Val loss is tensor([5.07001])\n",
      "\tTrain Epoch: 1 [0/2646 (0%)]\tLoss: 5.350194\n",
      "\tTrain Epoch: 1 [200/2646 (8%)]\tLoss: 2.228661\n",
      "\tTrain Epoch: 1 [400/2646 (15%)]\tLoss: 4.805711\n",
      "\tTrain Epoch: 1 [600/2646 (23%)]\tLoss: 6.353627\n",
      "\tTrain Epoch: 1 [800/2646 (30%)]\tLoss: 5.453632\n",
      "\tTrain Epoch: 1 [1000/2646 (38%)]\tLoss: 3.829545\n",
      "\tTrain Epoch: 1 [1200/2646 (45%)]\tLoss: 5.057739\n",
      "\tTrain Epoch: 1 [1400/2646 (53%)]\tLoss: 7.791821\n",
      "\tTrain Epoch: 1 [1600/2646 (60%)]\tLoss: 3.122992\n",
      "\tTrain Epoch: 1 [1800/2646 (68%)]\tLoss: 4.344544\n",
      "\tTrain Epoch: 1 [2000/2646 (76%)]\tLoss: 8.083722\n",
      "\tTrain Epoch: 1 [2200/2646 (83%)]\tLoss: 6.649722\n",
      "\tTrain Epoch: 1 [2400/2646 (91%)]\tLoss: 3.678948\n",
      "\tTrain Epoch: 1 [2600/2646 (98%)]\tLoss: 3.519396\n",
      "Evaluating after Epoch 1:\n",
      "Val loss is tensor([4.93052])\n",
      "\tTrain Epoch: 2 [0/2646 (0%)]\tLoss: 2.302781\n",
      "\tTrain Epoch: 2 [200/2646 (8%)]\tLoss: 4.009174\n",
      "\tTrain Epoch: 2 [400/2646 (15%)]\tLoss: 4.648047\n",
      "\tTrain Epoch: 2 [600/2646 (23%)]\tLoss: 3.313339\n",
      "\tTrain Epoch: 2 [800/2646 (30%)]\tLoss: 3.505194\n",
      "\tTrain Epoch: 2 [1000/2646 (38%)]\tLoss: 2.500694\n",
      "\tTrain Epoch: 2 [1200/2646 (45%)]\tLoss: 2.676809\n",
      "\tTrain Epoch: 2 [1400/2646 (53%)]\tLoss: 3.576557\n",
      "\tTrain Epoch: 2 [1600/2646 (60%)]\tLoss: 6.660134\n",
      "\tTrain Epoch: 2 [1800/2646 (68%)]\tLoss: 4.025148\n",
      "\tTrain Epoch: 2 [2000/2646 (76%)]\tLoss: 3.770503\n",
      "\tTrain Epoch: 2 [2200/2646 (83%)]\tLoss: 3.319247\n",
      "\tTrain Epoch: 2 [2400/2646 (91%)]\tLoss: 2.697490\n",
      "\tTrain Epoch: 2 [2600/2646 (98%)]\tLoss: 4.846491\n",
      "Evaluating after Epoch 2:\n",
      "Val loss is tensor([5.02964])\n",
      "\tTrain Epoch: 3 [0/2646 (0%)]\tLoss: 6.630862\n",
      "\tTrain Epoch: 3 [200/2646 (8%)]\tLoss: 4.389285\n",
      "\tTrain Epoch: 3 [400/2646 (15%)]\tLoss: 3.439884\n",
      "\tTrain Epoch: 3 [600/2646 (23%)]\tLoss: 4.080237\n",
      "\tTrain Epoch: 3 [800/2646 (30%)]\tLoss: 2.407230\n",
      "\tTrain Epoch: 3 [1000/2646 (38%)]\tLoss: 3.176584\n",
      "\tTrain Epoch: 3 [1200/2646 (45%)]\tLoss: 3.006848\n",
      "\tTrain Epoch: 3 [1400/2646 (53%)]\tLoss: 4.827259\n",
      "\tTrain Epoch: 3 [1600/2646 (60%)]\tLoss: 3.132758\n",
      "\tTrain Epoch: 3 [1800/2646 (68%)]\tLoss: 4.363088\n",
      "\tTrain Epoch: 3 [2000/2646 (76%)]\tLoss: 3.462890\n",
      "\tTrain Epoch: 3 [2200/2646 (83%)]\tLoss: 3.568316\n",
      "\tTrain Epoch: 3 [2400/2646 (91%)]\tLoss: 4.333907\n",
      "\tTrain Epoch: 3 [2600/2646 (98%)]\tLoss: 7.982218\n",
      "Evaluating after Epoch 3:\n",
      "Val loss is tensor([4.86619])\n",
      "\tTrain Epoch: 4 [0/2646 (0%)]\tLoss: 5.324774\n",
      "\tTrain Epoch: 4 [200/2646 (8%)]\tLoss: 3.574639\n",
      "\tTrain Epoch: 4 [400/2646 (15%)]\tLoss: 6.133883\n",
      "\tTrain Epoch: 4 [600/2646 (23%)]\tLoss: 5.184619\n",
      "\tTrain Epoch: 4 [800/2646 (30%)]\tLoss: 3.195806\n",
      "\tTrain Epoch: 4 [1000/2646 (38%)]\tLoss: 3.489611\n",
      "\tTrain Epoch: 4 [1200/2646 (45%)]\tLoss: 2.838524\n",
      "\tTrain Epoch: 4 [1400/2646 (53%)]\tLoss: 2.508790\n",
      "\tTrain Epoch: 4 [1600/2646 (60%)]\tLoss: 3.112169\n",
      "\tTrain Epoch: 4 [1800/2646 (68%)]\tLoss: 3.164549\n",
      "\tTrain Epoch: 4 [2000/2646 (76%)]\tLoss: 2.520422\n",
      "\tTrain Epoch: 4 [2200/2646 (83%)]\tLoss: 5.982090\n",
      "\tTrain Epoch: 4 [2400/2646 (91%)]\tLoss: 3.590669\n",
      "\tTrain Epoch: 4 [2600/2646 (98%)]\tLoss: 2.808120\n",
      "Evaluating after Epoch 4:\n",
      "Val loss is tensor([5.67037])\n",
      "\tTrain Epoch: 5 [0/2646 (0%)]\tLoss: 4.920654\n",
      "\tTrain Epoch: 5 [200/2646 (8%)]\tLoss: 3.181891\n",
      "\tTrain Epoch: 5 [400/2646 (15%)]\tLoss: 3.853612\n",
      "\tTrain Epoch: 5 [600/2646 (23%)]\tLoss: 4.356838\n",
      "\tTrain Epoch: 5 [800/2646 (30%)]\tLoss: 3.352922\n",
      "\tTrain Epoch: 5 [1000/2646 (38%)]\tLoss: 4.854831\n",
      "\tTrain Epoch: 5 [1200/2646 (45%)]\tLoss: 3.911002\n",
      "\tTrain Epoch: 5 [1400/2646 (53%)]\tLoss: 5.780709\n",
      "\tTrain Epoch: 5 [1600/2646 (60%)]\tLoss: 4.543668\n",
      "\tTrain Epoch: 5 [1800/2646 (68%)]\tLoss: 2.509357\n",
      "\tTrain Epoch: 5 [2000/2646 (76%)]\tLoss: 4.910426\n",
      "\tTrain Epoch: 5 [2200/2646 (83%)]\tLoss: 4.058271\n",
      "\tTrain Epoch: 5 [2400/2646 (91%)]\tLoss: 3.772461\n",
      "\tTrain Epoch: 5 [2600/2646 (98%)]\tLoss: 3.453428\n",
      "Evaluating after Epoch 5:\n",
      "Val loss is tensor([5.24001])\n",
      "\tTrain Epoch: 6 [0/2646 (0%)]\tLoss: 2.603688\n",
      "\tTrain Epoch: 6 [200/2646 (8%)]\tLoss: 3.500762\n",
      "\tTrain Epoch: 6 [400/2646 (15%)]\tLoss: 2.744118\n",
      "\tTrain Epoch: 6 [600/2646 (23%)]\tLoss: 2.362740\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "min_val_loss = np.inf\n",
    "#val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch, sixinput=False)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    #val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=False)\n",
    "    #print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model, sixinput=False)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, 'models/best_bounding_box_10e-5_5e-7.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)\n",
    "\n",
    "checkpoint = torch.load('models/best_bounding_box_10e-5_5e-7.pt')\n",
    "checkpoint['val_loss_hist'] = val_loss_hist\n",
    "#checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "torch.save(checkpoint, 'models/best_bounding_box_10e-5_5e-7.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/best_bounding_box_10e-4_5e-7.pt\n",
      "Best val loss: tensor([4.91705], device='cuda:0')\n",
      "\n",
      "models/best_bounding_box_10e-5_5e-7.pt\n",
      "Best val loss: tensor([6.75921], device='cuda:0')\n",
      "\n",
      "models/best_bounding_box_10e-6_5e-7.pt\n",
      "Best val loss: tensor([5.53980], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show hyperparameter tuning results\n",
    "for i in [4, 5, 6]:\n",
    "    path = 'models/best_bounding_box_10e-{}_5e-7.pt'.format(str(i))\n",
    "    checkpoint = torch.load(path)\n",
    "    print(path)\n",
    "    print(\"Best val loss: {}\\n\".format(checkpoint['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25777e+07 parameters, 6.25777e+07 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (Conv2d): Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): WeightedFeatureFusion()\n",
       "    (5): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): WeightedFeatureFusion()\n",
       "    (9): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): WeightedFeatureFusion()\n",
       "    (12): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): WeightedFeatureFusion()\n",
       "    (16): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): WeightedFeatureFusion()\n",
       "    (19): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): WeightedFeatureFusion()\n",
       "    (22): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): WeightedFeatureFusion()\n",
       "    (25): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): WeightedFeatureFusion()\n",
       "    (28): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): WeightedFeatureFusion()\n",
       "    (31): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): WeightedFeatureFusion()\n",
       "    (34): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): WeightedFeatureFusion()\n",
       "    (37): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): WeightedFeatureFusion()\n",
       "    (41): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): WeightedFeatureFusion()\n",
       "    (44): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): WeightedFeatureFusion()\n",
       "    (47): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): WeightedFeatureFusion()\n",
       "    (50): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (52): WeightedFeatureFusion()\n",
       "    (53): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): WeightedFeatureFusion()\n",
       "    (56): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): WeightedFeatureFusion()\n",
       "    (59): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): WeightedFeatureFusion()\n",
       "    (62): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (65): WeightedFeatureFusion()\n",
       "    (66): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): WeightedFeatureFusion()\n",
       "    (69): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): WeightedFeatureFusion()\n",
       "    (72): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): WeightedFeatureFusion()\n",
       "    (75): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    (79): FeatureConcat()\n",
       "    (80): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "    (81): FeatureConcat()\n",
       "    (82): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    (83): FeatureConcat()\n",
       "    (84): Sequential(\n",
       "      (Conv2d): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (87): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (89): YOLOLayer()\n",
       "    (90): FeatureConcat()\n",
       "    (91): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (92): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (93): FeatureConcat()\n",
       "    (94): Sequential(\n",
       "      (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (97): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (98): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (99): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (Conv2d): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (101): YOLOLayer()\n",
       "    (102): FeatureConcat()\n",
       "    (103): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (104): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (105): FeatureConcat()\n",
       "    (106): Sequential(\n",
       "      (Conv2d): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (107): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (108): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (109): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (110): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (111): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (112): Sequential(\n",
       "      (Conv2d): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (113): YOLOLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in a saved model\n",
    "checkpoint = torch.load('models/best_bounding_box_10e-5_5e-7.pt')\n",
    "model = Darknet(checkpoint['config'], verbose=False).cuda()\n",
    "model.load_state_dict(checkpoint`['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c34acbcd2fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Make input the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(valloader).next()\n",
    "model.eval()\n",
    "sample = torch.stack(sample).to(device)\n",
    "\n",
    "# Make input the correct shape\n",
    "batch_size = sample.shape[0]\n",
    "sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through model\n",
    "output = model(sample)\n",
    "\n",
    "# Apply non-max supression\n",
    "#Returns list of length batch_size, with each list element being a tensor of size nx6 (x1, y1, x2, y2, conf, cls) \n",
    "conf_thres = 0.06\n",
    "iou_thres = 0\n",
    "output = non_max_suppression(output[0], conf_thres, iou_thres,\n",
    "                                   multi_label=False, classes=None, )\n",
    "# Rescale from 306x256 to 80x80 (from -40 to 40)\n",
    "if output[0] != None:\n",
    "    for i in range(output[0].shape[0]):\n",
    "        # Get four corners\n",
    "        x1=output[0][i][0]\n",
    "        y1=output[0][i][1]\n",
    "        x2=output[0][i][2]\n",
    "        y2=output[0][i][3]\n",
    "        bounding_boxes[i, :, :] = torch.tensor([[x1, x1, x2, x2],\n",
    "                                                [y1, y2, y1, y2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store bounding boxes in the correct format\n",
    "bounding_boxes = torch.zeros((output[0].shape[0], 2, 4))\n",
    "#len(output[0])\n",
    "if output[0] != None:\n",
    "    for i in range(output[0].shape[0]):\n",
    "        # Get four corners\n",
    "        bounding_boxes[i, :, :] = torch.tensor([[output[0][i][0],  output[0][i][2], output[0][i][2],  output[0][i][0]],\n",
    "                                                [output[0][i][1],  output[0][i][1], output[0][i][3], output[0][i][3]]])\n",
    "# Truncate corners that are out of range\n",
    "bounding_boxes[bounding_boxes>40] = 40\n",
    "bounding_boxes[bounding_boxes<-40] = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for dimension 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f279f9d53890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# You can check the implementation of the draw box to understand how it works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdraw_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for dimension 0 with size 6"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(bounding_boxes):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
