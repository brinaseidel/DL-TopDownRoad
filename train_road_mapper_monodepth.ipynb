{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RoadMapper model \n",
    "\n",
    "This notebook trains a model to take in six images from the car's point of view, and output a bird's eye view map of the road.\n",
    "\n",
    "It currently uses a modified version of [HRNet](https://github.com/HRNet/HRNet-Semantic-Segmentation) (Wang et. al. 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "from hrnet import get_seg_model\n",
    "from hrnet_sixinput import get_seg_model_sixinput\n",
    "\n",
    "#!pip install yacs\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "import PIL.Image as pil\n",
    "\n",
    "from monodepth2.monodepth import MonoDepthEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/dy1078/dlproject/data'\n",
    "annotation_csv = '/scratch/dy1078/dlproject/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the labeled training data, and split into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenes from 106 - 133 are labeled\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes: 21 \n",
      "Val scenes: 7\n"
     ]
    }
   ],
   "source": [
    "# Split 75/25 into training and validation\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"/scratch/dy1078/monodepth2/models/weights_2/\"\n",
    "\n",
    "# encoder_path = os.path.join(model_name, \"encoder.pth\")\n",
    "# depth_decoder_path = os.path.join(model_name, \"depth.pth\")\n",
    "\n",
    "# # LOADING MODEL\n",
    "# encoder = networks.ResnetEncoder(18, False)\n",
    "# depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "# loaded_dict_enc = torch.load(encoder_path, map_location=device)\n",
    "# filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "# encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "# loaded_dict = torch.load(depth_decoder_path, map_location=device)\n",
    "# depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "# encoder.eval()\n",
    "# depth_decoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_height = loaded_dict_enc['height']\n",
    "# feed_width = loaded_dict_enc['width']\n",
    "\n",
    "# feed_height = 256\n",
    "# feed_width = 306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera = 'CAM_FRONT_RIGHT.jpeg'\n",
    "# image_path = \"/scratch/dy1078/dlproject/data/scene_3/sample_2/\" + camera\n",
    "\n",
    "# input_image = pil.open(image_path)\n",
    "\n",
    "# original_width, original_height = input_image.size\n",
    "# input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "# input_image_depth = torchvision.transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "# input_image_pytorch = torchvision.transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = encoder(input_image_depth)\n",
    "#     outputs = depth_decoder(features)\n",
    "\n",
    "# disp = outputs[(\"disp\", 0)]\n",
    "\n",
    "# disp_resized = torch.nn.functional.interpolate(disp,\n",
    "#     (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# # Saving colormapped depth image\n",
    "# disp_resized_np = disp_resized.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp_resized.shape\n",
    "# input_image_pytorch.shape\n",
    "# torch.cat((input_image_pytorch, disp_resized),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 306])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = 'CAM_FRONT_RIGHT.jpeg'\n",
    "sample_image_path = \"/scratch/dy1078/dlproject/data/scene_3/sample_2/\" + camera\n",
    "\n",
    "input_image = pil.open(sample_image_path)\n",
    "MonoDepthEstimator(\"/scratch/dy1078/monodepth2/models/mono_model/models/weights_13/\")(input_image).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = torchvision.transforms.ToTensor()\n",
    "transform = MonoDepthEstimator(\"/scratch/dy1078/monodepth2/models/mono_model/models/weights_13/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)\n",
    "#print(torch.stack(sample)[:, 1, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "# Note: I need to clean this up and move the whole config to a seperate file with the rest of it \n",
    "\n",
    "config = CN(new_allowed=True)\n",
    "config.defrost()\n",
    "config.merge_from_file(\"hrnet_config.yaml\")\n",
    "config.TRAIN.LR = 10e-4\n",
    "config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model = get_seg_model(config, depth = True)\n",
    "model = get_seg_model_sixinput(config, depth = True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize loss\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize optimizer\n",
    "optimizer = torch.optim.SGD([{'params':\n",
    "                                  filter(lambda p: p.requires_grad,\n",
    "                                         model.parameters()),\n",
    "                                  'lr': config.TRAIN.LR}],\n",
    "                                lr=config.TRAIN.LR,\n",
    "                                momentum=config.TRAIN.MOMENTUM,\n",
    "                                weight_decay=config.TRAIN.WD,\n",
    "                                nesterov=config.TRAIN.NESTEROV,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "        \n",
    "        # Send to device\n",
    "        sample, road_image = torch.stack(sample).to(device),  torch.stack(road_image).float().to(device) # takes only one image for now\n",
    "        \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "        \n",
    "        # Calculate loss and take step\n",
    "        loss = criterion(output, road_image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function\n",
    "def evaluate(val_loader, model, criterion, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        # send to device\n",
    "        sample, road_image = torch.stack(sample).to(device), torch.stack(road_image).float().to(device) # takes only one image for now\n",
    "        \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "            \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, road_image)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Calculate TP, FP, FN for threat score\n",
    "        output_class = (output > 0.5).float()\n",
    "        tp = tp + torch.sum(torch.mul(output_class, road_image))\n",
    "        fp = fp + torch.sum(torch.mul(torch.eq(output_class, 1), torch.eq(road_image, 0)))\n",
    "        fn = fn + torch.sum(torch.mul(torch.eq(output_class, 0), torch.eq(road_image, 1)))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    threat_score = tp/(tp+fp+fn)\n",
    "    \n",
    "    return loss, threat_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dy1078/.local/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/dy1078/.local/lib/python3.7/site-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 0.690290\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 0.388822\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 0.277390\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 0.549126\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 0.517624\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 0.503566\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 0.321921\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 0.420115\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 0.377207\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 0.270638\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 0.237930\n",
      "\tTrain Epoch: 0 [2200/2646 (83%)]\tLoss: 0.222057\n",
      "\tTrain Epoch: 0 [2400/2646 (91%)]\tLoss: 0.276370\n",
      "\tTrain Epoch: 0 [2600/2646 (98%)]\tLoss: 0.321790\n",
      "Evaluating after Epoch 0:\n",
      "Val loss is 0.453238, threat score is 0.596359\n",
      "\tTrain Epoch: 1 [0/2646 (0%)]\tLoss: 0.432293\n",
      "\tTrain Epoch: 1 [200/2646 (8%)]\tLoss: 0.401298\n",
      "\tTrain Epoch: 1 [400/2646 (15%)]\tLoss: 0.293539\n",
      "\tTrain Epoch: 1 [600/2646 (23%)]\tLoss: 0.487087\n",
      "\tTrain Epoch: 1 [800/2646 (30%)]\tLoss: 0.380227\n",
      "\tTrain Epoch: 1 [1000/2646 (38%)]\tLoss: 0.392443\n",
      "\tTrain Epoch: 1 [1200/2646 (45%)]\tLoss: 0.405553\n",
      "\tTrain Epoch: 1 [1400/2646 (53%)]\tLoss: 0.383486\n",
      "\tTrain Epoch: 1 [1600/2646 (60%)]\tLoss: 0.253312\n",
      "\tTrain Epoch: 1 [1800/2646 (68%)]\tLoss: 0.251760\n",
      "\tTrain Epoch: 1 [2000/2646 (76%)]\tLoss: 0.219314\n",
      "\tTrain Epoch: 1 [2200/2646 (83%)]\tLoss: 0.404981\n",
      "\tTrain Epoch: 1 [2400/2646 (91%)]\tLoss: 0.398082\n",
      "\tTrain Epoch: 1 [2600/2646 (98%)]\tLoss: 0.215708\n",
      "Evaluating after Epoch 1:\n",
      "Val loss is 0.498439, threat score is 0.604576\n",
      "\tTrain Epoch: 2 [0/2646 (0%)]\tLoss: 0.531409\n",
      "\tTrain Epoch: 2 [200/2646 (8%)]\tLoss: 0.416553\n",
      "\tTrain Epoch: 2 [400/2646 (15%)]\tLoss: 0.307869\n",
      "\tTrain Epoch: 2 [600/2646 (23%)]\tLoss: 0.269608\n",
      "\tTrain Epoch: 2 [800/2646 (30%)]\tLoss: 0.276460\n",
      "\tTrain Epoch: 2 [1000/2646 (38%)]\tLoss: 0.204550\n",
      "\tTrain Epoch: 2 [1200/2646 (45%)]\tLoss: 0.270314\n",
      "\tTrain Epoch: 2 [1400/2646 (53%)]\tLoss: 0.365826\n",
      "\tTrain Epoch: 2 [1600/2646 (60%)]\tLoss: 0.223002\n",
      "\tTrain Epoch: 2 [1800/2646 (68%)]\tLoss: 0.310605\n",
      "\tTrain Epoch: 2 [2000/2646 (76%)]\tLoss: 0.586987\n",
      "\tTrain Epoch: 2 [2200/2646 (83%)]\tLoss: 0.359259\n",
      "\tTrain Epoch: 2 [2400/2646 (91%)]\tLoss: 0.420369\n",
      "\tTrain Epoch: 2 [2600/2646 (98%)]\tLoss: 0.195909\n",
      "Evaluating after Epoch 2:\n",
      "Val loss is 0.402670, threat score is 0.649610\n",
      "\tTrain Epoch: 3 [0/2646 (0%)]\tLoss: 0.298185\n",
      "\tTrain Epoch: 3 [200/2646 (8%)]\tLoss: 0.296645\n",
      "\tTrain Epoch: 3 [400/2646 (15%)]\tLoss: 0.268124\n",
      "\tTrain Epoch: 3 [600/2646 (23%)]\tLoss: 0.205903\n",
      "\tTrain Epoch: 3 [800/2646 (30%)]\tLoss: 0.295496\n",
      "\tTrain Epoch: 3 [1000/2646 (38%)]\tLoss: 0.114126\n",
      "\tTrain Epoch: 3 [1200/2646 (45%)]\tLoss: 0.371899\n",
      "\tTrain Epoch: 3 [1400/2646 (53%)]\tLoss: 0.223311\n",
      "\tTrain Epoch: 3 [1600/2646 (60%)]\tLoss: 0.538420\n",
      "\tTrain Epoch: 3 [1800/2646 (68%)]\tLoss: 0.156594\n",
      "\tTrain Epoch: 3 [2000/2646 (76%)]\tLoss: 0.235898\n",
      "\tTrain Epoch: 3 [2200/2646 (83%)]\tLoss: 0.371332\n",
      "\tTrain Epoch: 3 [2400/2646 (91%)]\tLoss: 0.270831\n",
      "\tTrain Epoch: 3 [2600/2646 (98%)]\tLoss: 0.265056\n",
      "Evaluating after Epoch 3:\n",
      "Val loss is 0.406546, threat score is 0.644478\n",
      "\tTrain Epoch: 4 [0/2646 (0%)]\tLoss: 0.223741\n",
      "\tTrain Epoch: 4 [200/2646 (8%)]\tLoss: 0.293877\n",
      "\tTrain Epoch: 4 [400/2646 (15%)]\tLoss: 0.267638\n",
      "\tTrain Epoch: 4 [600/2646 (23%)]\tLoss: 0.261972\n",
      "\tTrain Epoch: 4 [800/2646 (30%)]\tLoss: 0.432421\n",
      "\tTrain Epoch: 4 [1000/2646 (38%)]\tLoss: 0.405577\n",
      "\tTrain Epoch: 4 [1200/2646 (45%)]\tLoss: 0.186978\n",
      "\tTrain Epoch: 4 [1400/2646 (53%)]\tLoss: 0.308630\n",
      "\tTrain Epoch: 4 [1600/2646 (60%)]\tLoss: 0.335876\n",
      "\tTrain Epoch: 4 [1800/2646 (68%)]\tLoss: 0.274118\n",
      "\tTrain Epoch: 4 [2000/2646 (76%)]\tLoss: 0.223196\n",
      "\tTrain Epoch: 4 [2200/2646 (83%)]\tLoss: 0.173076\n",
      "\tTrain Epoch: 4 [2400/2646 (91%)]\tLoss: 0.212208\n",
      "\tTrain Epoch: 4 [2600/2646 (98%)]\tLoss: 0.307443\n",
      "Evaluating after Epoch 4:\n",
      "Val loss is 0.408651, threat score is 0.664598\n",
      "\tTrain Epoch: 5 [0/2646 (0%)]\tLoss: 0.254987\n",
      "\tTrain Epoch: 5 [200/2646 (8%)]\tLoss: 0.166108\n",
      "\tTrain Epoch: 5 [400/2646 (15%)]\tLoss: 0.083876\n",
      "\tTrain Epoch: 5 [600/2646 (23%)]\tLoss: 0.342762\n",
      "\tTrain Epoch: 5 [800/2646 (30%)]\tLoss: 0.332575\n",
      "\tTrain Epoch: 5 [1000/2646 (38%)]\tLoss: 0.161486\n",
      "\tTrain Epoch: 5 [1200/2646 (45%)]\tLoss: 0.219215\n",
      "\tTrain Epoch: 5 [1400/2646 (53%)]\tLoss: 0.071482\n",
      "\tTrain Epoch: 5 [1600/2646 (60%)]\tLoss: 0.178924\n",
      "\tTrain Epoch: 5 [1800/2646 (68%)]\tLoss: 0.066328\n",
      "\tTrain Epoch: 5 [2000/2646 (76%)]\tLoss: 0.289180\n",
      "\tTrain Epoch: 5 [2200/2646 (83%)]\tLoss: 0.544309\n",
      "\tTrain Epoch: 5 [2400/2646 (91%)]\tLoss: 0.314841\n",
      "\tTrain Epoch: 5 [2600/2646 (98%)]\tLoss: 0.161751\n",
      "Evaluating after Epoch 5:\n",
      "Val loss is 0.381183, threat score is 0.671533\n",
      "\tTrain Epoch: 6 [0/2646 (0%)]\tLoss: 0.144915\n",
      "\tTrain Epoch: 6 [200/2646 (8%)]\tLoss: 0.065734\n",
      "\tTrain Epoch: 6 [400/2646 (15%)]\tLoss: 0.103680\n",
      "\tTrain Epoch: 6 [600/2646 (23%)]\tLoss: 0.123123\n",
      "\tTrain Epoch: 6 [800/2646 (30%)]\tLoss: 0.168461\n",
      "\tTrain Epoch: 6 [1000/2646 (38%)]\tLoss: 0.202364\n",
      "\tTrain Epoch: 6 [1200/2646 (45%)]\tLoss: 0.122273\n",
      "\tTrain Epoch: 6 [1400/2646 (53%)]\tLoss: 0.079613\n",
      "\tTrain Epoch: 6 [1600/2646 (60%)]\tLoss: 0.212057\n",
      "\tTrain Epoch: 6 [1800/2646 (68%)]\tLoss: 0.090787\n",
      "\tTrain Epoch: 6 [2000/2646 (76%)]\tLoss: 0.059259\n",
      "\tTrain Epoch: 6 [2200/2646 (83%)]\tLoss: 0.051770\n",
      "\tTrain Epoch: 6 [2400/2646 (91%)]\tLoss: 0.236841\n",
      "\tTrain Epoch: 6 [2600/2646 (98%)]\tLoss: 0.133413\n",
      "Evaluating after Epoch 6:\n",
      "Val loss is 0.423356, threat score is 0.682243\n",
      "\tTrain Epoch: 7 [0/2646 (0%)]\tLoss: 0.209248\n",
      "\tTrain Epoch: 7 [200/2646 (8%)]\tLoss: 0.129053\n",
      "\tTrain Epoch: 7 [400/2646 (15%)]\tLoss: 0.177633\n",
      "\tTrain Epoch: 7 [600/2646 (23%)]\tLoss: 0.089072\n",
      "\tTrain Epoch: 7 [800/2646 (30%)]\tLoss: 0.200023\n",
      "\tTrain Epoch: 7 [1000/2646 (38%)]\tLoss: 0.134456\n",
      "\tTrain Epoch: 7 [1200/2646 (45%)]\tLoss: 0.077273\n",
      "\tTrain Epoch: 7 [1400/2646 (53%)]\tLoss: 0.248614\n",
      "\tTrain Epoch: 7 [1600/2646 (60%)]\tLoss: 0.218994\n",
      "\tTrain Epoch: 7 [1800/2646 (68%)]\tLoss: 0.059550\n",
      "\tTrain Epoch: 7 [2000/2646 (76%)]\tLoss: 0.067332\n",
      "\tTrain Epoch: 7 [2200/2646 (83%)]\tLoss: 0.109217\n",
      "\tTrain Epoch: 7 [2400/2646 (91%)]\tLoss: 0.239221\n",
      "\tTrain Epoch: 7 [2600/2646 (98%)]\tLoss: 0.173384\n",
      "Evaluating after Epoch 7:\n",
      "Val loss is 0.430064, threat score is 0.697348\n",
      "\tTrain Epoch: 8 [0/2646 (0%)]\tLoss: 0.053100\n",
      "\tTrain Epoch: 8 [200/2646 (8%)]\tLoss: 0.101109\n",
      "\tTrain Epoch: 8 [400/2646 (15%)]\tLoss: 0.107076\n",
      "\tTrain Epoch: 8 [600/2646 (23%)]\tLoss: 0.164139\n",
      "\tTrain Epoch: 8 [800/2646 (30%)]\tLoss: 0.207193\n",
      "\tTrain Epoch: 8 [1000/2646 (38%)]\tLoss: 0.085437\n",
      "\tTrain Epoch: 8 [1200/2646 (45%)]\tLoss: 0.156234\n",
      "\tTrain Epoch: 8 [1400/2646 (53%)]\tLoss: 0.149265\n",
      "\tTrain Epoch: 8 [1600/2646 (60%)]\tLoss: 0.149971\n",
      "\tTrain Epoch: 8 [1800/2646 (68%)]\tLoss: 0.145846\n",
      "\tTrain Epoch: 8 [2000/2646 (76%)]\tLoss: 0.079937\n",
      "\tTrain Epoch: 8 [2200/2646 (83%)]\tLoss: 0.179303\n",
      "\tTrain Epoch: 8 [2400/2646 (91%)]\tLoss: 0.204493\n",
      "\tTrain Epoch: 8 [2600/2646 (98%)]\tLoss: 0.261985\n",
      "Evaluating after Epoch 8:\n",
      "Val loss is 0.425563, threat score is 0.693615\n",
      "\tTrain Epoch: 9 [0/2646 (0%)]\tLoss: 0.139701\n",
      "\tTrain Epoch: 9 [200/2646 (8%)]\tLoss: 0.109713\n",
      "\tTrain Epoch: 9 [400/2646 (15%)]\tLoss: 0.050204\n",
      "\tTrain Epoch: 9 [600/2646 (23%)]\tLoss: 0.060207\n",
      "\tTrain Epoch: 9 [800/2646 (30%)]\tLoss: 0.110571\n",
      "\tTrain Epoch: 9 [1000/2646 (38%)]\tLoss: 0.157159\n",
      "\tTrain Epoch: 9 [1200/2646 (45%)]\tLoss: 0.144001\n",
      "\tTrain Epoch: 9 [1400/2646 (53%)]\tLoss: 0.154380\n",
      "\tTrain Epoch: 9 [1600/2646 (60%)]\tLoss: 0.170496\n",
      "\tTrain Epoch: 9 [1800/2646 (68%)]\tLoss: 0.176179\n",
      "\tTrain Epoch: 9 [2000/2646 (76%)]\tLoss: 0.052274\n",
      "\tTrain Epoch: 9 [2200/2646 (83%)]\tLoss: 0.108673\n",
      "\tTrain Epoch: 9 [2400/2646 (91%)]\tLoss: 0.097284\n",
      "\tTrain Epoch: 9 [2600/2646 (98%)]\tLoss: 0.146401\n",
      "Evaluating after Epoch 9:\n",
      "Val loss is 0.454546, threat score is 0.693493\n",
      "\tTrain Epoch: 10 [0/2646 (0%)]\tLoss: 0.097368\n",
      "\tTrain Epoch: 10 [200/2646 (8%)]\tLoss: 0.091328\n",
      "\tTrain Epoch: 10 [400/2646 (15%)]\tLoss: 0.095210\n",
      "\tTrain Epoch: 10 [600/2646 (23%)]\tLoss: 0.080507\n",
      "\tTrain Epoch: 10 [800/2646 (30%)]\tLoss: 0.124799\n",
      "\tTrain Epoch: 10 [1000/2646 (38%)]\tLoss: 0.123970\n",
      "\tTrain Epoch: 10 [1200/2646 (45%)]\tLoss: 0.088501\n",
      "\tTrain Epoch: 10 [1400/2646 (53%)]\tLoss: 0.135034\n",
      "\tTrain Epoch: 10 [1600/2646 (60%)]\tLoss: 0.056064\n",
      "\tTrain Epoch: 10 [1800/2646 (68%)]\tLoss: 0.091118\n",
      "\tTrain Epoch: 10 [2000/2646 (76%)]\tLoss: 0.057004\n",
      "\tTrain Epoch: 10 [2200/2646 (83%)]\tLoss: 0.111117\n",
      "\tTrain Epoch: 10 [2400/2646 (91%)]\tLoss: 0.139969\n",
      "\tTrain Epoch: 10 [2600/2646 (98%)]\tLoss: 0.102225\n",
      "Evaluating after Epoch 10:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss is 0.539544, threat score is 0.698248\n",
      "\tTrain Epoch: 11 [0/2646 (0%)]\tLoss: 0.069949\n",
      "\tTrain Epoch: 11 [200/2646 (8%)]\tLoss: 0.057329\n",
      "\tTrain Epoch: 11 [400/2646 (15%)]\tLoss: 0.142656\n",
      "\tTrain Epoch: 11 [600/2646 (23%)]\tLoss: 0.212969\n",
      "\tTrain Epoch: 11 [800/2646 (30%)]\tLoss: 0.139316\n",
      "\tTrain Epoch: 11 [1000/2646 (38%)]\tLoss: 0.151920\n",
      "\tTrain Epoch: 11 [1200/2646 (45%)]\tLoss: 0.086967\n",
      "\tTrain Epoch: 11 [1400/2646 (53%)]\tLoss: 0.114253\n",
      "\tTrain Epoch: 11 [1600/2646 (60%)]\tLoss: 0.060740\n",
      "\tTrain Epoch: 11 [1800/2646 (68%)]\tLoss: 0.115255\n",
      "\tTrain Epoch: 11 [2000/2646 (76%)]\tLoss: 0.148925\n",
      "\tTrain Epoch: 11 [2200/2646 (83%)]\tLoss: 0.094183\n",
      "\tTrain Epoch: 11 [2400/2646 (91%)]\tLoss: 0.056417\n",
      "\tTrain Epoch: 11 [2600/2646 (98%)]\tLoss: 0.121203\n",
      "Evaluating after Epoch 11:\n",
      "Val loss is 0.515391, threat score is 0.708864\n",
      "\tTrain Epoch: 12 [0/2646 (0%)]\tLoss: 0.073502\n",
      "\tTrain Epoch: 12 [200/2646 (8%)]\tLoss: 0.118341\n",
      "\tTrain Epoch: 12 [400/2646 (15%)]\tLoss: 0.049472\n",
      "\tTrain Epoch: 12 [600/2646 (23%)]\tLoss: 0.114556\n",
      "\tTrain Epoch: 12 [800/2646 (30%)]\tLoss: 0.119368\n",
      "\tTrain Epoch: 12 [1000/2646 (38%)]\tLoss: 0.055596\n",
      "\tTrain Epoch: 12 [1200/2646 (45%)]\tLoss: 0.128896\n",
      "\tTrain Epoch: 12 [1400/2646 (53%)]\tLoss: 0.102156\n",
      "\tTrain Epoch: 12 [1600/2646 (60%)]\tLoss: 0.066519\n",
      "\tTrain Epoch: 12 [1800/2646 (68%)]\tLoss: 0.052671\n",
      "\tTrain Epoch: 12 [2000/2646 (76%)]\tLoss: 0.043178\n",
      "\tTrain Epoch: 12 [2200/2646 (83%)]\tLoss: 0.074609\n",
      "\tTrain Epoch: 12 [2400/2646 (91%)]\tLoss: 0.075480\n",
      "\tTrain Epoch: 12 [2600/2646 (98%)]\tLoss: 0.071387\n",
      "Evaluating after Epoch 12:\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "max_val_threat_score = 0\n",
    "val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, loss, epoch, sixinput=True)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=True)\n",
    "    print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_threat_score > max_val_threat_score:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_threat_score': val_threat_score,\n",
    "            }, 'models/best_road_mapper_lr10e-4_sixinput_depth.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    val_threat_score_hist.append(val_threat_score)\n",
    "\n",
    "checkpoint = torch.load('models/best_road_mapper_lr10e-4_sixinput_depth.pt')\n",
    "checkpoint['val_loss_hist'] = val_loss_hist\n",
    "checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "torch.save(checkpoint, 'models/best_road_mapper_lr10e-4_sixinput_depth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "checkpoint = torch.load('models/best_road_mapper_lr10e-5_sixinput.pt')\n",
    "model = get_seg_model_sixinput(checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss vs threat score during training\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_loss_hist'], label=\"Val Loss\", linewidth=3)\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_threat_score_hist'], label=\"Val Threat Score\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "\n",
    "# Visualize results\n",
    "sample, target, road_image = iter(valloader).next()\n",
    "image = sample[0][None, :, :, :]\n",
    "#plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "#plt.axis('off')\n",
    "# True road image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_image[0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image\n",
    "output = model(image.to(device))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output.cpu().detach().numpy().squeeze(0), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image, with threshold\n",
    "output_binary = output[0, :, :] > 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output_binary.cpu().detach().numpy(), cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate:  10e-4\n",
      "Sixinput:  False\n",
      "Model path:  models/best_road_mapper_lr10e-4.pt\n",
      "Validation threat score:  tensor(0.6924, device='cuda:0')\n",
      "\n",
      "Learning rate:  10e-4\n",
      "Sixinput:  True\n",
      "Model path:  models/best_road_mapper_lr10e-4_sixinput.pt\n",
      "Validation threat score:  tensor(0.6909, device='cuda:0')\n",
      "\n",
      "Learning rate:  10e-5\n",
      "Sixinput:  False\n",
      "Model path:  models/best_road_mapper_lr10e-5.pt\n",
      "Validation threat score:  tensor(0.7001, device='cuda:0')\n",
      "\n",
      "Learning rate:  10e-5\n",
      "Sixinput:  True\n",
      "Model path:  models/best_road_mapper_lr10e-5_sixinput.pt\n",
      "Validation threat score:  tensor(0.7083, device='cuda:0')\n",
      "\n",
      "Learning rate:  10e-6\n",
      "Sixinput:  False\n",
      "Model path:  models/best_road_mapper_lr10e-6.pt\n",
      "Validation threat score:  tensor(0.6925, device='cuda:0')\n",
      "\n",
      "Learning rate:  10e-6\n",
      "Sixinput:  True\n",
      "Model path:  models/best_road_mapper_lr10e-6_sixinput.pt\n",
      "Validation threat score:  tensor(0.6780, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check which model is best\n",
    "lrs = [\"10e-4\", \"10e-5\", \"10e-6\"]\n",
    "sixinput_opts = [\"\", \"_sixinput\"]\n",
    "\n",
    "for lr in lrs:\n",
    "    \n",
    "    for sixinput_opt in sixinput_opts:\n",
    "         \n",
    "        model_path = 'models/best_road_mapper_lr{}{}.pt'.format(lr, sixinput_opt)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        print(\"\\nLearning rate: \", lr)\n",
    "        print(\"Sixinput: \", \"True\"if sixinput_opt !=\"\" else \"False\")\n",
    "        print(\"Model path: \", model_path)\n",
    "        print(\"Validation threat score: \", checkpoint['val_threat_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
