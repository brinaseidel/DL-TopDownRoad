{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bounding Box model\n",
    "\n",
    "This notebook trains a model to take in six images from the car's point of view, and output a bird's eye view of the bouding boxes around surrounding objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge opencv -y\n",
    "# !conda install opencv -y\n",
    "import cv2 \n",
    "from yolov3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tqdm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install shapely --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/jm7519/.conda/envs/sdc3/lib/python3.6/site-packages/torch/version.py'>\n",
      "10.1\n",
      "7603\n"
     ]
    }
   ],
   "source": [
    "print(torch.version)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3380d2c12118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/sdc3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sdc3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m~/.conda/envs/sdc3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/jm7519/deeplearning/data'\n",
    "annotation_csv = '/scratch/jm7519/deeplearning/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the labeled training data, and split into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenes from 106 - 133 are labeled\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes: 21 \n",
      "Val scenes: 7\n"
     ]
    }
   ],
   "source": [
    "# Split 75/25 into training and validation\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# inference\n",
    "from monodepth2.monodepth import MonoDepthEstimator\n",
    "transform = MonoDepthEstimator(\"./models/weights_13/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=1, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)\n",
    "#print(torch.stack(sample)[:, 1, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "\n",
    "config = \"yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 2.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.0001,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.00005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25777e+07 parameters, 6.25777e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "model = Darknet(config, verbose=False).to(device)\n",
    "#ONNX_EXPORT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25794e+07 parameters, 6.25794e+07 gradients\n",
      "dict_keys(['epoch', 'config', 'model_state_dict', 'optimizer_state_dict', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (Conv2d): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): WeightedFeatureFusion()\n",
       "    (5): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): WeightedFeatureFusion()\n",
       "    (9): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): WeightedFeatureFusion()\n",
       "    (12): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): WeightedFeatureFusion()\n",
       "    (16): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): WeightedFeatureFusion()\n",
       "    (19): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): WeightedFeatureFusion()\n",
       "    (22): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): WeightedFeatureFusion()\n",
       "    (25): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): WeightedFeatureFusion()\n",
       "    (28): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): WeightedFeatureFusion()\n",
       "    (31): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): WeightedFeatureFusion()\n",
       "    (34): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): WeightedFeatureFusion()\n",
       "    (37): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): WeightedFeatureFusion()\n",
       "    (41): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): WeightedFeatureFusion()\n",
       "    (44): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): WeightedFeatureFusion()\n",
       "    (47): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): WeightedFeatureFusion()\n",
       "    (50): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (52): WeightedFeatureFusion()\n",
       "    (53): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): WeightedFeatureFusion()\n",
       "    (56): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): WeightedFeatureFusion()\n",
       "    (59): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): WeightedFeatureFusion()\n",
       "    (62): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (65): WeightedFeatureFusion()\n",
       "    (66): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): WeightedFeatureFusion()\n",
       "    (69): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): WeightedFeatureFusion()\n",
       "    (72): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): WeightedFeatureFusion()\n",
       "    (75): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    (79): FeatureConcat()\n",
       "    (80): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "    (81): FeatureConcat()\n",
       "    (82): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    (83): FeatureConcat()\n",
       "    (84): Sequential(\n",
       "      (Conv2d): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (87): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (89): YOLOLayer()\n",
       "    (90): FeatureConcat()\n",
       "    (91): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (92): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (93): FeatureConcat()\n",
       "    (94): Sequential(\n",
       "      (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (97): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (98): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (99): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (Conv2d): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (101): YOLOLayer()\n",
       "    (102): FeatureConcat()\n",
       "    (103): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (104): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (105): FeatureConcat()\n",
       "    (106): Sequential(\n",
       "      (Conv2d): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (107): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (108): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (109): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (110): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (111): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (112): Sequential(\n",
       "      (Conv2d): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (113): YOLOLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load state dict\n",
    "model = Darknet(config, verbose=False, depth=True).to(device)\n",
    "# checkpoint = torch.load('models/best_bounding_box_iou_0.300000_lr_0.000010_depth_1.pt')\n",
    "checkpoint = torch.load('models/best_bounding_box_iou_0.300000_lr_0.000010_depth_1.pt',map_location=torch.device('cpu'))\n",
    "print(checkpoint.keys())\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss_hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e2abad8be163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss_hist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss_hist'"
     ]
    }
   ],
   "source": [
    "checkpoint.keys()\n",
    "checkpoint['val_loss_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize optimizer\n",
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(pg0, lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "\n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "\n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function\n",
    "def evaluate(val_loader, model, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "         \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "        # Calculate loss\n",
    "        #print(output[0].shape)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 10.482473\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 4.747674\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 5.916343\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 9.429242\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 3.866343\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 3.437435\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 3.584432\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 6.046414\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 6.523919\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 6.981150\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 3.968273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-829f7b5a74ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msixinput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate at the end of the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-be9ef5d2a27e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch, sixinput)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Run through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Calculate loss and take step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                     \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                     \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' >> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' + '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer %g %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# WeightedFeatureFusion(), FeatureConcat()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YOLOLayer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 \u001b[0myolo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, outputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Adjust channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# slice input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m  \u001b[0;31m# or a = nn.ZeroPad2d((0, 0, 0, 0, 0, dc))(a); x = x + a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "min_val_loss = np.inf\n",
    "#val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch, sixinput=False)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    #val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=False)\n",
    "    #print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model, sixinput=False)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, 'models/best_bounding_box.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)\n",
    "    \n",
    "checkpoint = torch.load('models/best_bounding_box.pt')\n",
    "checkpoint['val_loss_hist'] = val_loss_hist\n",
    "#checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "torch.save(checkpoint, 'models/best_bounding_box.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import compute_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# %%timeit \n",
    "avg = 0\n",
    "conf_thres = 0.06\n",
    "\n",
    "initial = None\n",
    "counter = 1\n",
    "\n",
    "while True:\n",
    "    print(counter)\n",
    "    sample, target, road_image = iter(valloader).next()\n",
    "    if initial == sample:\n",
    "        break    \n",
    "    if initial is None:\n",
    "        initial = sample\n",
    "\n",
    "\n",
    "    counter += 1\n",
    "    model.eval()\n",
    "    sample = torch.stack(sample).to(device)\n",
    "\n",
    "    # Make input the correct shape\n",
    "    batch_size = sample.shape[0]\n",
    "    sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "    # Run through model\n",
    "    optimizer.zero_grad()\n",
    "    output_tmp = model(sample)\n",
    "    # output_tmp[0][1, :, 0].max()\n",
    "\n",
    "    # output_tmp[0].shape, output_tmp[1][0].shape, output_tmp[1][1].shape, output_tmp[1][2].shape\n",
    "\n",
    "    # from importlib import reload  \n",
    "    # reload(yolov3)\n",
    "\n",
    "    # Apply non-max supression\n",
    "    #Returns list of length batch_size, with each list element being a tensor of size nx6 (x1, y1, x2, y2, conf, cls) \n",
    "    # conf_thres = 0.07\n",
    "    # conf_thres = 0.06\n",
    "    # iou_thres = 0.6\n",
    "    iou_thres = 0\n",
    "    output = non_max_suppression(output_tmp[0], conf_thres, iou_thres,\n",
    "                                       multi_label=False, classes=None, )\n",
    "\n",
    "#     print(output[0].shape)\n",
    "#     print(output[1].shape)\n",
    "\n",
    "    # Rescale from 256x306 to 80x80 (from -40 to 40)\n",
    "    if output[0] is None:\n",
    "        output = output[1]\n",
    "    elif output[1] is None:\n",
    "        output = output[1]\n",
    "    elif output[0] is None and output[1] is None:\n",
    "        output = None\n",
    "        \n",
    "    if output is not None:    \n",
    "        for i, preds in enumerate(output):\n",
    "        #     print(i, preds)\n",
    "            output[i][:, 0] = preds[:, 0] * 80/306 - 40 # x1\n",
    "            output[i][:, 1] = preds[:, 1] * 80/256 - 40 # y1\n",
    "            output[i][:, 2] = preds[:, 2] * 80/306 - 40 # x2\n",
    "            output[i][:, 3] = preds[:, 3] * 80/256 - 40 # y2\n",
    "\n",
    "\n",
    "        # for i in range(4):\n",
    "        #     print(output[0][:, i].min(), output[0][:, i].max())\n",
    "\n",
    "\n",
    "        # Store bounding boxes in the correct format\n",
    "        bounding_boxes = torch.zeros((output[0].shape[0], 2, 4))\n",
    "        #len(output[0])\n",
    "        if output[0] != None:\n",
    "            for i in range(output[0].shape[0]):\n",
    "                # Get four corners\n",
    "        #         print(output[0][i])\n",
    "                x1=output[0][i][0]\n",
    "                y1=output[0][i][1]\n",
    "                x2=output[0][i][2]\n",
    "                y2=output[0][i][3]\n",
    "                bounding_boxes[i, :, :] = torch.tensor([[x1, x1, x2, x2],\n",
    "                                                        [y1, y2, y1, y2]])\n",
    "\n",
    "        #         break\n",
    "\n",
    "        # Truncate corners that are out of range\n",
    "        bounding_boxes[bounding_boxes>40] = 40\n",
    "        bounding_boxes[bounding_boxes<-40] = -40\n",
    "    else:\n",
    "        bounding_boxes = torch.zeros((1, 2, 4))\n",
    "    avg += compute_ats_bounding_boxes(target[0]['bounding_box'], bounding_boxes)\n",
    "print(avg / counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 2, 4))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes = torch.zeros((0, 2, 4))\n",
    "\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot perform reduction function max on tensor with no elements because the operation does not have an identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c42ca1304d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_ats_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bounding_box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/helper.py\u001b[0m in \u001b[0;36mcompute_ats_bounding_boxes\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mboxes1_min_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mboxes2_max_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mboxes2_min_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mboxes2_max_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot perform reduction function max on tensor with no elements because the operation does not have an identity"
     ]
    }
   ],
   "source": [
    "compute_ats_bounding_boxes(target[0]['bounding_box'], bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " tensor([[4.53425e+01, 1.24534e+02, 6.28699e+01, 1.30833e+02, 3.33078e-01, 0.00000e+00],\n",
       "         [5.31911e+01, 1.36277e+02, 7.03936e+01, 1.42443e+02, 1.85911e-01, 0.00000e+00],\n",
       "         [1.77796e+02, 1.36894e+02, 1.95715e+02, 1.42783e+02, 1.47606e-01, 0.00000e+00],\n",
       "         [1.60112e+02, 1.36920e+02, 1.77728e+02, 1.43068e+02, 1.46965e-01, 0.00000e+00],\n",
       "         [5.75759e+01, 2.09322e+02, 7.95611e+01, 2.16214e+02, 9.46838e-02, 0.00000e+00],\n",
       "         [3.40118e+01, 1.36666e+02, 5.20758e+01, 1.43093e+02, 7.65677e-02, 0.00000e+00],\n",
       "         [1.92822e+02, 1.36763e+02, 2.10864e+02, 1.42548e+02, 7.35049e-02, 0.00000e+00]], grad_fn=<IndexBackward>)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(bounding_boxes):\n",
    "#     print(bb)\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][0]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "checkpoint = torch.load('models/best_road_mapper_lr10e-5_sixinput.pt')\n",
    "model = get_seg_model_sixinput(checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs threat score during training\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_loss_hist'], label=\"Val Loss\", linewidth=3)\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_threat_score_hist'], label=\"Val Threat Score\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "\n",
    "# Visualize results\n",
    "sample, target, road_image = iter(valloader).next()\n",
    "image = sample[0][None, :, :, :]\n",
    "#plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "#plt.axis('off')\n",
    "# True road image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_image[0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image\n",
    "output = model(image.to(device))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output.cpu().detach().numpy().squeeze(0), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image, with threshold\n",
    "output_binary = output[0, :, :] > 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output_binary.cpu().detach().numpy(), cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
