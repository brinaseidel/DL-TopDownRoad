{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bounding Box model\n",
    "\n",
    "This notebook trains a model to take in six images from the car's point of view, and output a bird's eye view of the bouding boxes around surrounding objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge opencv -y\n",
    "# !conda install opencv -y\n",
    "import cv2 \n",
    "from yolov3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tqdm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install shapely --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/jm7519/.conda/envs/sdc3/lib/python3.6/site-packages/torch/version.py'>\n",
      "10.1\n",
      "7603\n"
     ]
    }
   ],
   "source": [
    "print(torch.version)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/jm7519/deeplearning/data'\n",
    "annotation_csv = '/scratch/jm7519/deeplearning/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the labeled training data, and split into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenes from 106 - 133 are labeled\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes: 21 \n",
      "Val scenes: 7\n"
     ]
    }
   ],
   "source": [
    "# Split 75/25 into training and validation\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# inference\n",
    "from monodepth2.monodepth import MonoDepthEstimator\n",
    "transform = MonoDepthEstimator(\"./models/weights_13/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)\n",
    "#print(torch.stack(sample)[:, 1, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "\n",
    "config = \"yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 2.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.0001,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.00005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "model = Darknet(config, verbose=False).to(device)\n",
    "#ONNX_EXPORT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25794e+07 parameters, 6.25794e+07 gradients\n",
      "dict_keys(['epoch', 'config', 'model_state_dict', 'optimizer_state_dict', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (Conv2d): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): WeightedFeatureFusion()\n",
       "    (5): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): WeightedFeatureFusion()\n",
       "    (9): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): WeightedFeatureFusion()\n",
       "    (12): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): WeightedFeatureFusion()\n",
       "    (16): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): WeightedFeatureFusion()\n",
       "    (19): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): WeightedFeatureFusion()\n",
       "    (22): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): WeightedFeatureFusion()\n",
       "    (25): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): WeightedFeatureFusion()\n",
       "    (28): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): WeightedFeatureFusion()\n",
       "    (31): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): WeightedFeatureFusion()\n",
       "    (34): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): WeightedFeatureFusion()\n",
       "    (37): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): WeightedFeatureFusion()\n",
       "    (41): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): WeightedFeatureFusion()\n",
       "    (44): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): WeightedFeatureFusion()\n",
       "    (47): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): WeightedFeatureFusion()\n",
       "    (50): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (52): WeightedFeatureFusion()\n",
       "    (53): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): WeightedFeatureFusion()\n",
       "    (56): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): WeightedFeatureFusion()\n",
       "    (59): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): WeightedFeatureFusion()\n",
       "    (62): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (65): WeightedFeatureFusion()\n",
       "    (66): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): WeightedFeatureFusion()\n",
       "    (69): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): WeightedFeatureFusion()\n",
       "    (72): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): WeightedFeatureFusion()\n",
       "    (75): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    (79): FeatureConcat()\n",
       "    (80): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "    (81): FeatureConcat()\n",
       "    (82): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    (83): FeatureConcat()\n",
       "    (84): Sequential(\n",
       "      (Conv2d): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (87): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (89): YOLOLayer()\n",
       "    (90): FeatureConcat()\n",
       "    (91): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (92): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (93): FeatureConcat()\n",
       "    (94): Sequential(\n",
       "      (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (97): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (98): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (99): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (Conv2d): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (101): YOLOLayer()\n",
       "    (102): FeatureConcat()\n",
       "    (103): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (104): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (105): FeatureConcat()\n",
       "    (106): Sequential(\n",
       "      (Conv2d): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (107): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (108): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (109): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (110): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (111): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (112): Sequential(\n",
       "      (Conv2d): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (113): YOLOLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load state dict\n",
    "model = Darknet(config, verbose=False, depth=True).to(device)\n",
    "checkpoint = torch.load('models/best_bounding_box_iou_0.300000_lr_0.000010_depth_1.pt')\n",
    "print(checkpoint.keys())\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss_hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e2abad8be163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss_hist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss_hist'"
     ]
    }
   ],
   "source": [
    "checkpoint.keys()\n",
    "checkpoint['val_loss_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize optimizer\n",
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(pg0, lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "\n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "\n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function\n",
    "def evaluate(val_loader, model, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "         \n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            batch_size = sample.shape[0]\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "        # Calculate loss\n",
    "        #print(output[0].shape)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 10.482473\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 4.747674\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 5.916343\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 9.429242\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 3.866343\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 3.437435\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 3.584432\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 6.046414\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 6.523919\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 6.981150\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 3.968273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-829f7b5a74ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msixinput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate at the end of the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-be9ef5d2a27e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch, sixinput)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Run through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Calculate loss and take step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                     \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                     \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' >> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' + '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer %g %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# WeightedFeatureFusion(), FeatureConcat()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YOLOLayer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 \u001b[0myolo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jm7519/deeplearning/DL-TopDownRoad/yolov3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, outputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Adjust channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# slice input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m  \u001b[0;31m# or a = nn.ZeroPad2d((0, 0, 0, 0, 0, dc))(a); x = x + a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "min_val_loss = np.inf\n",
    "#val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch, sixinput=False)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    #val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=False)\n",
    "    #print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model, sixinput=False)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, 'models/best_bounding_box.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)\n",
    "    \n",
    "checkpoint = torch.load('models/best_bounding_box.pt')\n",
    "checkpoint['val_loss_hist'] = val_loss_hist\n",
    "#checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "torch.save(checkpoint, 'models/best_bounding_box.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample, target, road_image = iter(valloader).next()\n",
    "model.eval()\n",
    "sample = torch.stack(sample).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4, 256, 306])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input the correct shape\n",
    "batch_size = sample.shape[0]\n",
    "sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "# Run through model\n",
    "optimizer.zero_grad()\n",
    "output_tmp = model(sample)\n",
    "# output_tmp[0][1, :, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4944, 6]),\n",
       " torch.Size([2, 3, 8, 10, 6]),\n",
       " torch.Size([2, 3, 16, 20, 6]),\n",
       " torch.Size([2, 3, 32, 39, 6]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tmp[0].shape, output_tmp[1][0].shape, output_tmp[1][1].shape, output_tmp[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 6])\n",
      "torch.Size([22, 6])\n"
     ]
    }
   ],
   "source": [
    "# Apply non-max supression\n",
    "#Returns list of length batch_size, with each list element being a tensor of size nx6 (x1, y1, x2, y2, conf, cls) \n",
    "conf_thres = 0.07\n",
    "iou_thres = 0.2\n",
    "# iou_thres = 0.3\n",
    "output = non_max_suppression(output_tmp[0], conf_thres, iou_thres,\n",
    "                                   multi_label=False, classes=None, )\n",
    "\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)\n",
    "\n",
    "# Rescale from 256x306 to 80x80 (from -40 to 40)\n",
    "for i, preds in enumerate(output):\n",
    "#     print(i, preds)\n",
    "    output[i][:, 0] = preds[:, 0] * 80/306 - 40 # x1\n",
    "    output[i][:, 1] = preds[:, 1] * 80/256 - 40 # y1\n",
    "    output[i][:, 2] = preds[:, 2] * 80/306 - 40 # x2\n",
    "    output[i][:, 3] = preds[:, 3] * 80/256 - 40 # y2\n",
    "#     output[i][:, 0] = preds[:, 0] # x1\n",
    "#     output[i][:, 1] = preds[:, 1] # y1\n",
    "#     output[i][:, 2] = preds[:, 2] # x2\n",
    "#     output[i][:, 3] = preds[:, 3] # y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.33023, device='cuda:0', grad_fn=<MinBackward1>) tensor(36.07819, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(-4.62313, device='cuda:0', grad_fn=<MinBackward1>) tensor(25.36612, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(-34.33787, device='cuda:0', grad_fn=<MinBackward1>) tensor(40.33999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(-2.78843, device='cuda:0', grad_fn=<MinBackward1>) tensor(27.50093, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(output[0][:, i].min(), output[0][:, i].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-16.09755,  -4.21482, -11.23014,  -2.29709,   0.42545,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-26.84195,  -4.12864, -21.73085,  -2.09413,   0.30625,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-14.57354,  -4.25550,  -9.38204,  -2.24786,   0.27194,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-22.33795,  -4.07475, -17.58828,  -2.20145,   0.19762,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-20.40956,  -4.17014, -15.38191,  -2.16990,   0.19630,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([11.54949,  2.56358, 15.68052,  4.44412,  0.19577,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-18.03229,  -4.14076, -13.26132,  -2.25209,   0.19177,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-12.48806,  -4.22090,  -7.42488,  -2.12136,   0.18029,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-33.49300,  24.97443, -28.10858,  27.11748,   0.16546,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-31.42064,  24.93761, -26.08017,  27.01253,   0.16531,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-24.47818,  -4.12736, -19.71826,  -2.12817,   0.16492,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([4.75280, 2.61876, 9.42261, 4.63180, 0.16388, 0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-35.52739,  25.17484, -29.99196,  27.16718,   0.15956,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-39.33023,  25.21715, -34.05159,  27.40754,   0.14941,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-29.16669,  24.85163, -24.09757,  27.01138,   0.14292,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 2.52154, -4.50074,  7.69267, -2.39839,  0.14268,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([20.99055, 25.04491, 26.31015, 27.22025,  0.13798,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([25.15829, 25.19809, 30.64542, 27.39940,  0.13179,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-27.22629,  24.86184, -21.82858,  27.01984,   0.13027,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 4.53827, -4.47330,  9.47680, -2.44969,  0.12807,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 8.97178,  2.51828, 13.61343,  4.44722,  0.12665,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-37.88787,  25.06437, -32.06929,  27.31386,   0.12556,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 6.44716, -4.51534, 11.24486, -2.63560,  0.12298,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([33.55640, 25.19800, 38.95183, 27.40524,  0.12116,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 0.17517, -4.41460,  5.34334, -2.40079,  0.11052,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 6.91414,  2.63071, 11.57654,  4.52953,  0.10884,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([27.34413, 25.36612, 32.66327, 27.45300,  0.10864,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([23.23707, 25.17171, 28.13478, 27.30512,  0.10850,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-14.29014,  25.13017,  -9.13903,  27.18507,   0.10256,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-16.66900,  25.06063, -10.85350,  27.19618,   0.10198,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-35.26444,  -4.15862, -30.31723,  -2.11120,   0.10169,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([18.72360, 25.06059, 24.37209, 27.32126,  0.10128,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-10.35070,  -4.18002,  -5.59216,  -2.18507,   0.10082,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-24.91401,  24.81934, -19.68602,  27.01691,   0.09656,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([29.31628, 25.27320, 34.74424, 27.31749,  0.09509,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-18.60243,  24.94182, -12.67473,  27.16649,   0.08987,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-29.06885,  -4.13697, -23.92206,  -2.20499,   0.08957,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 8.70018, -4.62313, 13.43429, -2.78843,  0.08417,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 4.61325, 24.97572,  9.48231, 27.50093,  0.08383,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([2.82748, 2.86384, 7.59917, 4.77882, 0.08364, 0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-22.87986,  24.89764, -17.90640,  27.06541,   0.08299,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([13.40142,  2.63226, 18.17234,  4.52526,  0.07938,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([25.68158,  2.51836, 30.46046,  4.29226,  0.07787,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([31.39935, 25.10772, 36.92342, 27.34003,  0.07785,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([27.88584,  2.30759, 32.35040,  4.32645,  0.07772,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([29.75258,  6.16274, 34.47003,  8.19686,  0.07746,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([31.80214,  2.40785, 36.39878,  4.39748,  0.07714,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-39.26593,  -4.42817, -34.33787,  -2.27858,   0.07196,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-20.79604,  24.91258, -15.51336,  27.12849,   0.07134,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-31.04457,  -4.09994, -25.96385,  -2.14161,   0.07105,   0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([36.07819,  2.76908, 40.33999,  4.54870,  0.07022,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-3.74649, -4.30774,  1.33351, -2.26139,  0.06929,  0.00000], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Store bounding boxes in the correct format\n",
    "bounding_boxes = torch.zeros((output[0].shape[0], 2, 4))\n",
    "#len(output[0])\n",
    "if output[0] != None:\n",
    "    for i in range(output[0].shape[0]):\n",
    "        # Get four corners\n",
    "#         print(output[0][i])\n",
    "        x1=output[0][i][0]\n",
    "        y1=output[0][i][1]\n",
    "        x2=output[0][i][2]\n",
    "        y2=output[0][i][3]\n",
    "        bounding_boxes[i, :, :] = torch.tensor([[x1, x1, x2, x2],\n",
    "                                                [y1, y2, y1, y2]])\n",
    "\n",
    "#         break\n",
    "        \n",
    "# Truncate corners that are out of range\n",
    "    bounding_boxes[bounding_boxes>40] = 40\n",
    "    bounding_boxes[bounding_boxes<-40] = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(bounding_boxes):\n",
    "#     print(bb)\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][0]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "checkpoint = torch.load('models/best_road_mapper_lr10e-5_sixinput.pt')\n",
    "model = get_seg_model_sixinput(checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs threat score during training\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_loss_hist'], label=\"Val Loss\", linewidth=3)\n",
    "plt.plot(list(range(len(checkpoint['val_loss_hist']))), checkpoint['val_threat_score_hist'], label=\"Val Threat Score\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "\n",
    "# Visualize results\n",
    "sample, target, road_image = iter(valloader).next()\n",
    "image = sample[0][None, :, :, :]\n",
    "#plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "#plt.axis('off')\n",
    "# True road image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_image[0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image\n",
    "output = model(image.to(device))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output.cpu().detach().numpy().squeeze(0), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted road image, with threshold\n",
    "output_binary = output[0, :, :] > 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(output_binary.cpu().detach().numpy(), cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
